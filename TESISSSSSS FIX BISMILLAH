{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3GdmNoDGK2Ac+SpXTBOtP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesicha24001-ship-it/klasifikasi-credit-smote-nc-dan-bo/blob/main/TESISSSSSS%20FIX%20BISMILLAH\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell #1: Install semua library dulu\n",
        "!pip install bayesian-optimization -q\n",
        "!pip install imbalanced-learn -q\n",
        "!pip install xgboost -q\n",
        "\n",
        "print(\"‚úÖ Instalasi selesai!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHNHmkcnicDl",
        "outputId": "d9a42b8e-3b80-4319-f9df-1e8fd61a0c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Instalasi selesai!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell #2: Import semua library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, recall_score,\n",
        "                             precision_score, f1_score, roc_auc_score,\n",
        "                             average_precision_score, make_scorer)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create results directory\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "\n",
        "print(\"‚úÖ Import berhasil!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "oJUv1-mgio3j",
        "outputId": "fab8b0c0-2f6d-4af8-fa5b-ab04cdec80f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bayes_opt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2415051242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeometric_mean_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbayes_opt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bayes_opt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cbeac66",
        "outputId": "42651cfe-99d0-42e4-bc87-8fb8cb60760e"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TESIS KLASIFIKASI KELAYAKAN KREDIT DENGAN SMOTE-NC DAN BAYESIAN OPTIMIZATION v8.2 (FIXED)\n",
        "\n",
        "Author: Jesicha Aulia Adam (diperbaiki oleh AI Assistant)\n",
        "Version: 8.2 - PERBAIKAN CRITICAL: sampling_strategy di-optimize!\n",
        "Date: January 2026\n",
        "\n",
        "PERBAIKAN UTAMA v8.2:\n",
        "- sampling_strategy kini di-optimize oleh Bayesian Optimization (0.5 - 1.0)\n",
        "- random_state=None untuk variasi synthetic data\n",
        "- Tambahan logging untuk debugging\n",
        "- Verifikasi perbedaan hasil antara Default dan BO\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "#                               IMPORT LIBRARIES\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, accuracy_score, recall_score,\n",
        "    precision_score, f1_score, roc_auc_score,\n",
        "    roc_curve, average_precision_score, make_scorer\n",
        ")\n",
        "from sklearn.base import clone\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except ImportError:\n",
        "    print(\"XGBoost tidak terinstal. Install dengan: pip install xgboost\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Imbalanced-learn\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTENC\n",
        "    from imblearn.metrics import geometric_mean_score\n",
        "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "except ImportError:\n",
        "    print(\"Imbalanced-learn tidak terinstal. Menginstal sekarang...\")\n",
        "    !pip install imbalanced-learn\n",
        "    try: # Re-attempt import after installation\n",
        "        from imblearn.over_sampling import SMOTENC\n",
        "        from imblearn.metrics import geometric_mean_score\n",
        "        from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "    except ImportError:\n",
        "        print(\"Gagal menginstal atau mengimpor Imbalanced-learn. Harap periksa koneksi atau log instalasi.\")\n",
        "        sys.exit(1) # Exit if re-import also fails\n",
        "\n",
        "# Bayesian Optimization\n",
        "try:\n",
        "    from bayes_opt import BayesianOptimization\n",
        "except ImportError:\n",
        "    print(\"Bayesian Optimization tidak terinstal. Menginstal sekarang...\")\n",
        "    !pip install bayesian-optimization\n",
        "    try: # Re-attempt import after installation\n",
        "        from bayes_opt import BayesianOptimization\n",
        "    except ImportError:\n",
        "        print(\"Gagal menginstal atau mengimpor Bayesian Optimization. Harap periksa koneksi atau log instalasi.\")\n",
        "        sys.exit(1) # Exit if re-import also fails\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Create results directory\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "\n",
        "# ==============================================================================\n",
        "#                                   KONFIGURASI\n",
        "# ==============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    'data_file': '/content/Data bank sulteng 2 TESIS.csv',\n",
        "    'test_size': 0.3,\n",
        "    'random_state': 42,\n",
        "    'bo_init_points': 5,\n",
        "    'bo_n_iter': 20,\n",
        "    'cv_folds': 3,  # StratifiedKFold untuk BO\n",
        "    'results_dir': 'results'\n",
        "}\n",
        "\n",
        "# ==============================================================================\n",
        "#                               UTILITY FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def print_header(text, width=90):\n",
        "    print(\"=\" * width)\n",
        "    print(text.center(width))\n",
        "    print(\"=\" * width)\n",
        "\n",
        "def print_subheader(text):\n",
        "    print(f\"\\n{'='*90}\")\n",
        "    print(f\" {text}\")\n",
        "    print(f\"{'='*90}\")\n",
        "\n",
        "def find_column(df, possible_names):\n",
        "    \"\"\"Find column by possible names\"\"\"\n",
        "    df_cols_lower = [col.lower().strip() for col in df.columns]\n",
        "    for name in possible_names:\n",
        "        if name.lower().strip() in df_cols_lower:\n",
        "            idx = df_cols_lower.index(name.lower().strip())\n",
        "            return df.columns[idx]\n",
        "    return None\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba=None, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Protect if confusion matrix not 2x2\n",
        "    if cm.size == 1:\n",
        "        tn = cm.flat[0]\n",
        "        fp = fn = tp = 0\n",
        "    else:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    sensitivity = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "    specificity = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    try:\n",
        "        g_mean = geometric_mean_score(y_true, y_pred)\n",
        "    except Exception:\n",
        "        g_mean = np.nan\n",
        "\n",
        "    if y_pred_proba is not None:\n",
        "        auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
        "        auc_pr = average_precision_score(y_true, y_pred_proba)\n",
        "    else:\n",
        "        auc_roc = None\n",
        "        auc_pr = None\n",
        "\n",
        "    unique_preds = len(np.unique(y_pred))\n",
        "    is_broken = (unique_preds < 2)\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Sensitivity': sensitivity,\n",
        "        'Specificity': specificity,\n",
        "        'Precision': precision,\n",
        "        'F1-Score': f1,\n",
        "        'G-Mean': g_mean,\n",
        "        'AUC-ROC': auc_roc,\n",
        "        'AUC-PR': auc_pr,\n",
        "        'TP': int(tp), 'TN': int(tn), 'FP': int(fp), 'FN': int(fn),\n",
        "        'Broken': is_broken,\n",
        "        'N_Classes_Predicted': unique_preds\n",
        "    }, cm\n",
        "\n",
        "def print_comparison_table(results_df, title=\"PERBANDINGAN MODEL\"):\n",
        "    \"\"\"Print comparison table\"\"\"\n",
        "    print(f\"\\n{'='*90}\")\n",
        "    print(f\"{title:^90}\")\n",
        "    print(f\"{'='*90}\")\n",
        "\n",
        "    metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1-Score', 'G-Mean', 'AUC-ROC']\n",
        "\n",
        "    print(f\"{'Model':<45}\", end='')\n",
        "    for metric in metrics:\n",
        "        print(f\"{metric:>12}\", end='')\n",
        "    print()\n",
        "    print(\"-\" * 139)\n",
        "\n",
        "    for _, row in results_df.iterrows():\n",
        "        print(f\"{row['Model']:<45}\", end='')\n",
        "        for metric in metrics:\n",
        "            if pd.notna(row[metric]):\n",
        "                print(f\"{row[metric]:>12.4f}\", end='')\n",
        "            else:\n",
        "                print(f\"{'N/A':>12}\", end='')\n",
        "        print()\n",
        "\n",
        "def save_results_to_csv(results_list, filename):\n",
        "    \"\"\"Save results to CSV\"\"\"\n",
        "    df = pd.DataFrame(results_list)\n",
        "    filepath = os.path.join(CONFIG['results_dir'], filename)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(f\"‚úì Results saved: {filepath}\")\n",
        "    return df\n",
        "\n",
        "def plot_data_distribution(y_data, title, target_mapping, filename=None, subtitle=None):\n",
        "    \"\"\"Plot data distribution\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    dist = pd.Series(y_data).value_counts().sort_index()\n",
        "    colors = ['#3498db', '#e74c3c']\n",
        "    labels_names = [target_mapping[i] for i in sorted(dist.index)]\n",
        "    counts = [dist[i] for i in sorted(dist.index)]\n",
        "\n",
        "    # Bar chart\n",
        "    bars = ax1.bar(labels_names, counts, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    ax1.set_ylabel('Jumlah Sampel', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Distribusi Kelas', fontsize=13, fontweight='bold')\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "    # Pie chart\n",
        "    ax2.pie(counts, labels=labels_names, autopct='%1.1f%%', colors=colors,\n",
        "            startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
        "    ax2.set_title('Proporsi Kelas', fontsize=13, fontweight='bold')\n",
        "\n",
        "    if subtitle:\n",
        "        fig.suptitle(f\"{title}\\n{subtitle}\", fontsize=15, fontweight='bold', y=1.02)\n",
        "    else:\n",
        "        fig.suptitle(title, fontsize=15, fontweight='bold', y=1.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if filename:\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        print(f\"‚úì Visualisasi distribusi disimpan: {filename}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\nSTATISTIK DISTRIBUSI:\")\n",
        "    total = len(y_data)\n",
        "    for label in sorted(dist.index):\n",
        "        count = dist[label]\n",
        "        pct = (count / total) * 100\n",
        "        label_name = target_mapping[label]\n",
        "        print(f\"  {label_name:<20} {count:>6} sampel ({pct:>5.1f}%)\")\n",
        "\n",
        "    if len(dist) == 2:\n",
        "        ratio = dist.max() / dist.min()\n",
        "        print(f\"  Imbalance Ratio: {ratio:.2f}:1\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                         BAYESIAN OPTIMIZATION HELPERS\n",
        "# ==============================================================================\n",
        "\n",
        "# Make G-Mean scorer\n",
        "g_mean_scorer = make_scorer(\n",
        "    lambda y_true, y_pred: geometric_mean_score(y_true, y_pred),\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "def cv_score_on_resampled(model, X_res, y_res, cv_splits=3):\n",
        "    \"\"\"Compute CV mean G-Mean on resampled data with StratifiedKFold\"\"\"\n",
        "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=CONFIG['random_state'])\n",
        "    try:\n",
        "        scores = cross_val_score(model, X_res, y_res, scoring=g_mean_scorer, cv=cv, n_jobs=-1)\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning CV error: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# ==============================================================================\n",
        "#                               MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    print_header(\"TESIS KLASIFIKASI KELAYAKAN KREDIT\")\n",
        "    print(\" \" * 20 + \"Fixed Sampling Strategy + Bayesian Optimization v8.2 (FIXED)\")\n",
        "    print(\" \" * 32 + \"VERSION 8.2 - CRITICAL BUG FIXED!\")\n",
        "    print(\" \" * 28 + f\"Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bayesian Optimization tidak terinstal. Menginstal sekarang...\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-3.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (25.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.6.0)\n",
            "Downloading bayesian_optimization-3.2.0-py3-none-any.whl (37 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-3.2.0 colorama-0.4.6\n",
            "==========================================================================================\n",
            "                            TESIS KLASIFIKASI KELAYAKAN KREDIT                            \n",
            "==========================================================================================\n",
            "                    Fixed Sampling Strategy + Bayesian Optimization v8.2 (FIXED)\n",
            "                                VERSION 8.2 - CRITICAL BUG FIXED!\n",
            "                            Started: 2026-01-11 14:52:56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell #3: Konfigurasi\n",
        "CONFIG = {\n",
        "    'data_file': 'content/Data bank sulteng 2 TESIS.csv',  # ‚Üê Sesuaikan path Anda\n",
        "    'test_size': 0.3,\n",
        "    'random_state': 42,\n",
        "    'bo_init_points': 5,\n",
        "    'bo_n_iter': 20,\n",
        "    'cv_folds': 3,\n",
        "    'results_dir': 'results'\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Konfigurasi selesai!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifLg4M9ri9dC",
        "outputId": "962709f4-217f-47c7-c1ae-09f607b59d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Konfigurasi selesai!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CARA 1: UPLOAD FILE MANUAL\n",
        "# ==============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "print(\"üì§ Silakan upload file CSV Anda...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get filename\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"\\n‚úÖ File uploaded: {filename}\")\n",
        "\n",
        "# Read CSV\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]), sep=';')\n",
        "print(f\"‚úÖ Data loaded: {df.shape}\")\n",
        "print(f\"‚úÖ Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Lanjutkan dengan kode preprocessing...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "HKvlcAvUl393",
        "outputId": "c9a1d6b8-43fc-42fa-e511-cceb6dd7bd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Silakan upload file CSV Anda...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6394ccd6-a0ea-40d2-9d9d-8f32a6b6ffea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6394ccd6-a0ea-40d2-9d9d-8f32a6b6ffea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Data bank sulteng 2 TESIS.csv to Data bank sulteng 2 TESIS (1).csv\n",
            "\n",
            "‚úÖ File uploaded: Data bank sulteng 2 TESIS (1).csv\n",
            "‚úÖ Data loaded: (1448, 8)\n",
            "‚úÖ Columns: ['No', 'Y', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# COMPLETE PREPROCESSING - SETELAH FILE DI-UPLOAD\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" COMPLETE PREPROCESSING\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. DEFINE TARGET & FEATURES\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nüéØ STEP 1: DEFINE TARGET & FEATURES\")\n",
        "\n",
        "target_col = 'Y'  # Kolom target\n",
        "print(f\"‚úÖ Target column: {target_col}\")\n",
        "\n",
        "# Remove 'No' column if exists (biasanya index)\n",
        "if 'No' in df.columns:\n",
        "    df = df.drop(columns=['No'])\n",
        "    print(\"‚úÖ Column 'No' dropped (index column)\")\n",
        "\n",
        "print(f\"‚úÖ Working with {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. IDENTIFY FEATURE TYPES\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nüîç STEP 2: IDENTIFY FEATURE TYPES\")\n",
        "\n",
        "categorical_features = df.select_dtypes(include='object').columns.tolist()\n",
        "if target_col in categorical_features:\n",
        "    categorical_features.remove(target_col)\n",
        "\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "if target_col in numerical_features:\n",
        "    numerical_features.remove(target_col)\n",
        "\n",
        "print(f\"‚úÖ Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "print(f\"‚úÖ Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. ENCODE CATEGORICAL FEATURES\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nüîÑ STEP 3: ENCODE CATEGORICAL FEATURES\")\n",
        "\n",
        "df_processed = df.copy()\n",
        "le_dict = {}\n",
        "\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df_processed[col] = le.fit_transform(df[col].astype(str))\n",
        "    le_dict[col] = le\n",
        "    print(f\"  ‚úì '{col}' encoded: {len(le.classes_)} categories ‚Üí {le.classes_}\")\n",
        "\n",
        "# Encode target\n",
        "le_target = LabelEncoder()\n",
        "df_processed[target_col] = le_target.fit_transform(df[target_col].astype(str))\n",
        "target_mapping = {i: label for i, label in enumerate(le_target.classes_)}\n",
        "\n",
        "print(f\"\\n‚úÖ TARGET MAPPING:\")\n",
        "for code, label in target_mapping.items():\n",
        "    count = (df_processed[target_col] == code).sum()\n",
        "    pct = (count / len(df_processed)) * 100\n",
        "    print(f\"  {code} ‚Üí {label}: {count} samples ({pct:.1f}%)\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. SPLIT FEATURES & TARGET\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n‚úÇÔ∏è STEP 4: SPLIT FEATURES & TARGET\")\n",
        "\n",
        "X = df_processed.drop(columns=[target_col])\n",
        "y = df_processed[target_col]\n",
        "\n",
        "print(f\"‚úÖ X shape: {X.shape}\")\n",
        "print(f\"‚úÖ y shape: {y.shape}\")\n",
        "print(f\"‚úÖ y unique values: {np.unique(y)}\")\n",
        "\n",
        "# Get categorical indices for SMOTE-NC\n",
        "categorical_indices = [X.columns.get_loc(col) for col in categorical_features if col in X.columns]\n",
        "print(f\"\\n‚úÖ Categorical indices for SMOTE-NC: {categorical_indices}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. TRAIN-TEST SPLIT\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n‚úÇÔ∏è STEP 5: TRAIN-TEST SPLIT (70:30)\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"‚úÖ Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
        "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
        "\n",
        "print(f\"\\n‚úÖ Training distribution:\")\n",
        "for label in sorted(train_dist.index):\n",
        "    train_count = train_dist[label]\n",
        "    test_count = test_dist.get(label, 0)\n",
        "    label_name = target_mapping[label]\n",
        "    print(f\"  {label_name}: {train_count} (train) + {test_count} (test) = {train_count + test_count} total\")\n",
        "\n",
        "min_class_count = train_dist.min()\n",
        "max_k_neighbors = min(min_class_count - 1, 10)\n",
        "imbalance_ratio = train_dist.max() / train_dist.min()\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è Imbalance Ratio (Training): {imbalance_ratio:.2f}:1\")\n",
        "print(f\"‚úÖ Max k_neighbors for SMOTE-NC: {max_k_neighbors}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# VERIFICATION\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" ‚úÖ‚úÖ‚úÖ PREPROCESSING COMPLETE! ‚úÖ‚úÖ‚úÖ\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(\"\\nüìã AVAILABLE VARIABLES:\")\n",
        "print(f\"  ‚úì df_processed: {df_processed.shape}\")\n",
        "print(f\"  ‚úì target_col: '{target_col}'\")\n",
        "print(f\"  ‚úì categorical_features: {categorical_features}\")\n",
        "print(f\"  ‚úì numerical_features: {numerical_features}\")\n",
        "print(f\"  ‚úì categorical_indices: {categorical_indices}\")\n",
        "print(f\"  ‚úì target_mapping: {target_mapping}\")\n",
        "print(f\"  ‚úì X_train: {X_train.shape}\")\n",
        "print(f\"  ‚úì X_test: {X_test.shape}\")\n",
        "print(f\"  ‚úì y_train: {y_train.shape}\")\n",
        "print(f\"  ‚úì y_test: {y_test.shape}\")\n",
        "print(f\"  ‚úì max_k_neighbors: {max_k_neighbors}\")\n",
        "print(f\"  ‚úì imbalance_ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "print(\"\\nüöÄ Siap untuk modeling!\")\n",
        "print(\"=\"*90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co0VINg-mKLt",
        "outputId": "480b67a2-d327-4c31-db7c-183f08ea81de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " COMPLETE PREPROCESSING\n",
            "==========================================================================================\n",
            "\n",
            "üéØ STEP 1: DEFINE TARGET & FEATURES\n",
            "‚úÖ Target column: Y\n",
            "‚úÖ Column 'No' dropped (index column)\n",
            "‚úÖ Working with 1448 rows √ó 7 columns\n",
            "\n",
            "üîç STEP 2: IDENTIFY FEATURE TYPES\n",
            "‚úÖ Categorical features (3): ['X1', 'X2', 'X6']\n",
            "‚úÖ Numerical features (3): ['X3', 'X4', 'X5']\n",
            "\n",
            "üîÑ STEP 3: ENCODE CATEGORICAL FEATURES\n",
            "  ‚úì 'X1' encoded: 4 categories ‚Üí ['D3' 'S1' 'S2' 'SMA']\n",
            "  ‚úì 'X2' encoded: 2 categories ‚Üí ['NON PNS' 'PNS']\n",
            "  ‚úì 'X6' encoded: 2 categories ‚Üí ['Female' 'Male']\n",
            "\n",
            "‚úÖ TARGET MAPPING:\n",
            "  0 ‚Üí Diterima: 1183 samples (81.7%)\n",
            "  1 ‚Üí Ditolak: 265 samples (18.3%)\n",
            "\n",
            "‚úÇÔ∏è STEP 4: SPLIT FEATURES & TARGET\n",
            "‚úÖ X shape: (1448, 6)\n",
            "‚úÖ y shape: (1448,)\n",
            "‚úÖ y unique values: [0 1]\n",
            "\n",
            "‚úÖ Categorical indices for SMOTE-NC: [0, 1, 5]\n",
            "\n",
            "‚úÇÔ∏è STEP 5: TRAIN-TEST SPLIT (70:30)\n",
            "‚úÖ Training set: 1013 samples (70.0%)\n",
            "‚úÖ Test set: 435 samples (30.0%)\n",
            "\n",
            "‚úÖ Training distribution:\n",
            "  Diterima: 828 (train) + 355 (test) = 1183 total\n",
            "  Ditolak: 185 (train) + 80 (test) = 265 total\n",
            "\n",
            "‚öñÔ∏è Imbalance Ratio (Training): 4.48:1\n",
            "‚úÖ Max k_neighbors for SMOTE-NC: 10\n",
            "\n",
            "==========================================================================================\n",
            " ‚úÖ‚úÖ‚úÖ PREPROCESSING COMPLETE! ‚úÖ‚úÖ‚úÖ\n",
            "==========================================================================================\n",
            "\n",
            "üìã AVAILABLE VARIABLES:\n",
            "  ‚úì df_processed: (1448, 7)\n",
            "  ‚úì target_col: 'Y'\n",
            "  ‚úì categorical_features: ['X1', 'X2', 'X6']\n",
            "  ‚úì numerical_features: ['X3', 'X4', 'X5']\n",
            "  ‚úì categorical_indices: [0, 1, 5]\n",
            "  ‚úì target_mapping: {0: 'Diterima', 1: 'Ditolak'}\n",
            "  ‚úì X_train: (1013, 6)\n",
            "  ‚úì X_test: (435, 6)\n",
            "  ‚úì y_train: (1013,)\n",
            "  ‚úì y_test: (435,)\n",
            "  ‚úì max_k_neighbors: 10\n",
            "  ‚úì imbalance_ratio: 4.48:1\n",
            "\n",
            "üöÄ Siap untuk modeling!\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BASELINE MODELS (No SMOTE)\n",
        "# ==============================================================================\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, recall_score,\n",
        "                             precision_score, f1_score, roc_auc_score,\n",
        "                             average_precision_score)\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba=None, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if cm.size == 1:\n",
        "        tn = cm.flat[0]\n",
        "        fp = fn = tp = 0\n",
        "    else:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    sensitivity = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "    specificity = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    g_mean = geometric_mean_score(y_true, y_pred)\n",
        "\n",
        "    if y_pred_proba is not None:\n",
        "        auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
        "        auc_pr = average_precision_score(y_true, y_pred_proba)\n",
        "    else:\n",
        "        auc_roc = None\n",
        "        auc_pr = None\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Sensitivity': sensitivity,\n",
        "        'Specificity': specificity,\n",
        "        'Precision': precision,\n",
        "        'F1-Score': f1,\n",
        "        'G-Mean': g_mean,\n",
        "        'AUC-ROC': auc_roc,\n",
        "        'AUC-PR': auc_pr\n",
        "    }\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" STEP 4: BASELINE MODELS (No SMOTE)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "baseline_results = []\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"\\nüîπ Training: Logistic Regression (Baseline)...\")\n",
        "lr_base = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_base.fit(X_train, y_train)\n",
        "y_pred = lr_base.predict(X_test)\n",
        "y_pred_proba = lr_base.predict_proba(X_test)[:, 1]\n",
        "result_lr = evaluate_model(y_test, y_pred, y_pred_proba, \"Logistic Regression (Baseline)\")\n",
        "baseline_results.append(result_lr)\n",
        "print(f\"  ‚úì Accuracy: {result_lr['Accuracy']:.4f} | G-Mean: {result_lr['G-Mean']:.4f} | AUC-ROC: {result_lr['AUC-ROC']:.4f}\")\n",
        "\n",
        "# AdaBoost\n",
        "print(\"\\nüîπ Training: AdaBoost (Baseline)...\")\n",
        "ada_base = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "ada_base.fit(X_train, y_train)\n",
        "y_pred = ada_base.predict(X_test)\n",
        "y_pred_proba = ada_base.predict_proba(X_test)[:, 1]\n",
        "result_ada = evaluate_model(y_test, y_pred, y_pred_proba, \"AdaBoost (Baseline)\")\n",
        "baseline_results.append(result_ada)\n",
        "print(f\"  ‚úì Accuracy: {result_ada['Accuracy']:.4f} | G-Mean: {result_ada['G-Mean']:.4f} | AUC-ROC: {result_ada['AUC-ROC']:.4f}\")\n",
        "\n",
        "# XGBoost\n",
        "print(\"\\nüîπ Training: XGBoost (Baseline)...\")\n",
        "xgb_base = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
        "xgb_base.fit(X_train, y_train)\n",
        "y_pred = xgb_base.predict(X_test)\n",
        "y_pred_proba = xgb_base.predict_proba(X_test)[:, 1]\n",
        "result_xgb = evaluate_model(y_test, y_pred, y_pred_proba, \"XGBoost (Baseline)\")\n",
        "baseline_results.append(result_xgb)\n",
        "print(f\"  ‚úì Accuracy: {result_xgb['Accuracy']:.4f} | G-Mean: {result_xgb['G-Mean']:.4f} | AUC-ROC: {result_xgb['AUC-ROC']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" ‚úÖ BASELINE MODELS SELESAI!\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Display results\n",
        "df_baseline = pd.DataFrame(baseline_results)\n",
        "print(\"\\nüìä HASIL BASELINE:\")\n",
        "print(df_baseline[['Model', 'Accuracy', 'Sensitivity', 'Specificity', 'G-Mean', 'AUC-ROC']].to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LTe7KzqmMNe",
        "outputId": "0a44e2be-1ca2-41da-e480-3e17a26d2794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " STEP 4: BASELINE MODELS (No SMOTE)\n",
            "==========================================================================================\n",
            "\n",
            "üîπ Training: Logistic Regression (Baseline)...\n",
            "  ‚úì Accuracy: 0.8161 | G-Mean: 0.0000 | AUC-ROC: 0.6144\n",
            "\n",
            "üîπ Training: AdaBoost (Baseline)...\n",
            "  ‚úì Accuracy: 0.8276 | G-Mean: 0.5430 | AUC-ROC: 0.7915\n",
            "\n",
            "üîπ Training: XGBoost (Baseline)...\n",
            "  ‚úì Accuracy: 0.7862 | G-Mean: 0.5472 | AUC-ROC: 0.7136\n",
            "\n",
            "==========================================================================================\n",
            " ‚úÖ BASELINE MODELS SELESAI!\n",
            "==========================================================================================\n",
            "\n",
            "üìä HASIL BASELINE:\n",
            "                         Model  Accuracy  Sensitivity  Specificity   G-Mean  AUC-ROC\n",
            "Logistic Regression (Baseline)  0.816092       0.0000     1.000000 0.000000 0.614366\n",
            "           AdaBoost (Baseline)  0.827586       0.3125     0.943662 0.543042 0.791532\n",
            "            XGBoost (Baseline)  0.786207       0.3375     0.887324 0.547240 0.713556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SMOTE-NC DEFAULT\n",
        "# ==============================================================================\n",
        "\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" STEP 5: SMOTE-NC DEFAULT (sampling_strategy=0.8, k_neighbors=5)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Apply SMOTE-NC with default settings\n",
        "print(\"\\nüîÑ Applying SMOTE-NC Default...\")\n",
        "smote_default = SMOTENC(\n",
        "    categorical_features=categorical_indices,\n",
        "    sampling_strategy=0.8,\n",
        "    k_neighbors=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_smote_default, y_train_smote_default = smote_default.fit_resample(X_train, y_train)\n",
        "\n",
        "dist_after = pd.Series(y_train_smote_default).value_counts().sort_index()\n",
        "print(f\"\\n‚úÖ Distribution after SMOTE-NC Default:\")\n",
        "for label in sorted(dist_after.index):\n",
        "    count = dist_after[label]\n",
        "    label_name = target_mapping[label]\n",
        "    print(f\"  {label_name}: {count} samples\")\n",
        "\n",
        "new_ir = dist_after.max() / dist_after.min()\n",
        "print(f\"  Imbalance Ratio: {new_ir:.2f}:1\")\n",
        "print(f\"  Total training samples: {len(y_train_smote_default)} (was {len(y_train)})\")\n",
        "\n",
        "smote_default_results = []\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"\\nüîπ Training: Logistic Regression (SMOTE-NC Default)...\")\n",
        "lr_smote = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_smote.fit(X_train_smote_default, y_train_smote_default)\n",
        "y_pred = lr_smote.predict(X_test)\n",
        "y_pred_proba = lr_smote.predict_proba(X_test)[:, 1]\n",
        "result = evaluate_model(y_test, y_pred, y_pred_proba, \"Logistic Regression (SMOTE-NC Default)\")\n",
        "smote_default_results.append(result)\n",
        "print(f\"  ‚úì Accuracy: {result['Accuracy']:.4f} | G-Mean: {result['G-Mean']:.4f} | AUC-ROC: {result['AUC-ROC']:.4f}\")\n",
        "\n",
        "# AdaBoost\n",
        "print(\"\\nüîπ Training: AdaBoost (SMOTE-NC Default)...\")\n",
        "ada_smote = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "ada_smote.fit(X_train_smote_default, y_train_smote_default)\n",
        "y_pred = ada_smote.predict(X_test)\n",
        "y_pred_proba = ada_smote.predict_proba(X_test)[:, 1]\n",
        "result = evaluate_model(y_test, y_pred, y_pred_proba, \"AdaBoost (SMOTE-NC Default)\")\n",
        "smote_default_results.append(result)\n",
        "print(f\"  ‚úì Accuracy: {result['Accuracy']:.4f} | G-Mean: {result['G-Mean']:.4f} | AUC-ROC: {result['AUC-ROC']:.4f}\")\n",
        "\n",
        "# XGBoost\n",
        "print(\"\\nüîπ Training: XGBoost (SMOTE-NC Default)...\")\n",
        "xgb_smote = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
        "xgb_smote.fit(X_train_smote_default, y_train_smote_default)\n",
        "y_pred = xgb_smote.predict(X_test)\n",
        "y_pred_proba = xgb_smote.predict_proba(X_test)[:, 1]\n",
        "result = evaluate_model(y_test, y_pred, y_pred_proba, \"XGBoost (SMOTE-NC Default)\")\n",
        "smote_default_results.append(result)\n",
        "print(f\"  ‚úì Accuracy: {result['Accuracy']:.4f} | G-Mean: {result['G-Mean']:.4f} | AUC-ROC: {result['AUC-ROC']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" ‚úÖ SMOTE-NC DEFAULT SELESAI!\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Display results\n",
        "df_smote_default = pd.DataFrame(smote_default_results)\n",
        "print(\"\\nüìä HASIL SMOTE-NC DEFAULT:\")\n",
        "print(df_smote_default[['Model', 'Accuracy', 'Sensitivity', 'Specificity', 'G-Mean', 'AUC-ROC']].to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1JELiSVmpzJ",
        "outputId": "f9b23a30-decf-4cd1-eadc-6db018adfd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " STEP 5: SMOTE-NC DEFAULT (sampling_strategy=0.8, k_neighbors=5)\n",
            "==========================================================================================\n",
            "\n",
            "üîÑ Applying SMOTE-NC Default...\n",
            "\n",
            "‚úÖ Distribution after SMOTE-NC Default:\n",
            "  Diterima: 828 samples\n",
            "  Ditolak: 662 samples\n",
            "  Imbalance Ratio: 1.25:1\n",
            "  Total training samples: 1490 (was 1013)\n",
            "\n",
            "üîπ Training: Logistic Regression (SMOTE-NC Default)...\n",
            "  ‚úì Accuracy: 0.7011 | G-Mean: 0.5839 | AUC-ROC: 0.6236\n",
            "\n",
            "üîπ Training: AdaBoost (SMOTE-NC Default)...\n",
            "  ‚úì Accuracy: 0.8138 | G-Mean: 0.7376 | AUC-ROC: 0.8136\n",
            "\n",
            "üîπ Training: XGBoost (SMOTE-NC Default)...\n",
            "  ‚úì Accuracy: 0.7540 | G-Mean: 0.6229 | AUC-ROC: 0.7238\n",
            "\n",
            "==========================================================================================\n",
            " ‚úÖ SMOTE-NC DEFAULT SELESAI!\n",
            "==========================================================================================\n",
            "\n",
            "üìä HASIL SMOTE-NC DEFAULT:\n",
            "                                 Model  Accuracy  Sensitivity  Specificity   G-Mean  AUC-ROC\n",
            "Logistic Regression (SMOTE-NC Default)  0.701149       0.4500     0.757746 0.583940 0.623592\n",
            "           AdaBoost (SMOTE-NC Default)  0.813793       0.6375     0.853521 0.737645 0.813592\n",
            "            XGBoost (SMOTE-NC Default)  0.754023       0.4750     0.816901 0.622919 0.723838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BAYESIAN OPTIMIZATION - LOGISTIC REGRESSION (FIXED!)\n",
        "# ==============================================================================\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# G-Mean scorer\n",
        "g_mean_scorer = make_scorer(\n",
        "    lambda y_true, y_pred: geometric_mean_score(y_true, y_pred),\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "def cv_score_on_resampled(model, X_res, y_res, cv_splits=3):\n",
        "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
        "    try:\n",
        "        scores = cross_val_score(model, X_res, y_res, scoring=g_mean_scorer, cv=cv, n_jobs=-1)\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" STEP 6: BAYESIAN OPTIMIZATION - LOGISTIC REGRESSION (FIXED!)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(\"\\nüîß PERBAIKAN KRITIS:\")\n",
        "print(\"  ‚úì sampling_strategy kini di-optimize (0.5 - 1.0)\")\n",
        "print(\"  ‚úì random_state=None untuk variasi synthetic data\")\n",
        "\n",
        "# Objective function (FIXED!)\n",
        "def lr_objective(k_neighbors, sampling_strategy, C):\n",
        "    k = int(round(k_neighbors))\n",
        "\n",
        "    # FIXED: sampling_strategy di-optimize!\n",
        "    smote = SMOTENC(\n",
        "        categorical_features=categorical_indices,\n",
        "        sampling_strategy=sampling_strategy,  # ‚Üê OPTIMIZE!\n",
        "        k_neighbors=k,\n",
        "        random_state=None  # ‚Üê None untuk variasi!\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "        model = LogisticRegression(C=C, max_iter=1000, random_state=42)\n",
        "        score = cv_score_on_resampled(model, X_res, y_res, cv_splits=3)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "# Bayesian Optimization\n",
        "lr_pbounds = {\n",
        "    'k_neighbors': (3, max_k_neighbors),\n",
        "    'sampling_strategy': (0.5, 1.0),  # ‚Üê BARU! Di-optimize!\n",
        "    'C': (1e-3, 100)\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Parameter Bounds:\")\n",
        "print(f\"  k_neighbors: (3, {max_k_neighbors})\")\n",
        "print(f\"  sampling_strategy: (0.5, 1.0)  ‚Üê BARU!\")\n",
        "print(f\"  C: (0.001, 100)\")\n",
        "\n",
        "print(f\"\\nüöÄ Starting Bayesian Optimization...\")\n",
        "print(f\"  Init Points: 5, Iterations: 20\")\n",
        "print(f\"  (Proses ini memakan waktu 2-5 menit...)\\n\")\n",
        "\n",
        "lr_optimizer = BayesianOptimization(\n",
        "    f=lr_objective,\n",
        "    pbounds=lr_pbounds,\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "lr_optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "lr_best_params = lr_optimizer.max['params']\n",
        "lr_best_score = lr_optimizer.max['target']\n",
        "\n",
        "print(f\"\\n‚úÖ BEST PARAMETERS:\")\n",
        "print(f\"  CV G-Mean: {lr_best_score:.4f}\")\n",
        "print(f\"  k_neighbors: {int(round(lr_best_params['k_neighbors']))}\")\n",
        "print(f\"  sampling_strategy: {lr_best_params['sampling_strategy']:.4f}  ‚Üê BERBEDA dari 0.8!\")\n",
        "print(f\"  C: {lr_best_params['C']:.4f}\")\n",
        "\n",
        "# Train final model\n",
        "print(f\"\\nüîÑ Training final model with best parameters...\")\n",
        "smote_lr_best = SMOTENC(\n",
        "    categorical_features=categorical_indices,\n",
        "    sampling_strategy=lr_best_params['sampling_strategy'],\n",
        "    k_neighbors=int(round(lr_best_params['k_neighbors'])),\n",
        "    random_state=42\n",
        ")\n",
        "X_train_lr_bo, y_train_lr_bo = smote_lr_best.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"  ‚úì Samples after SMOTE: {len(y_train_lr_bo)}\")\n",
        "\n",
        "lr_final = LogisticRegression(C=lr_best_params['C'], max_iter=1000, random_state=42)\n",
        "lr_final.fit(X_train_lr_bo, y_train_lr_bo)\n",
        "\n",
        "y_pred_lr_bo = lr_final.predict(X_test)\n",
        "y_pred_proba_lr_bo = lr_final.predict_proba(X_test)[:, 1]\n",
        "\n",
        "result_lr_bo = evaluate_model(y_test, y_pred_lr_bo, y_pred_proba_lr_bo, \"LogisticRegression (Fixed Sampling + BO)\")\n",
        "\n",
        "print(f\"\\n‚úÖ FINAL TEST PERFORMANCE:\")\n",
        "print(f\"  Accuracy: {result_lr_bo['Accuracy']:.4f}\")\n",
        "print(f\"  G-Mean: {result_lr_bo['G-Mean']:.4f}\")\n",
        "print(f\"  AUC-ROC: {result_lr_bo['AUC-ROC']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" ‚úÖ LOGISTIC REGRESSION BO SELESAI!\")\n",
        "print(\"=\"*90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ3nSko3mvcw",
        "outputId": "155a06cf-070e-43cc-8290-0648662c7b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " STEP 6: BAYESIAN OPTIMIZATION - LOGISTIC REGRESSION (FIXED!)\n",
            "==========================================================================================\n",
            "\n",
            "üîß PERBAIKAN KRITIS:\n",
            "  ‚úì sampling_strategy kini di-optimize (0.5 - 1.0)\n",
            "  ‚úì random_state=None untuk variasi synthetic data\n",
            "\n",
            "‚öôÔ∏è Parameter Bounds:\n",
            "  k_neighbors: (3, 10)\n",
            "  sampling_strategy: (0.5, 1.0)  ‚Üê BARU!\n",
            "  C: (0.001, 100)\n",
            "\n",
            "üöÄ Starting Bayesian Optimization...\n",
            "  Init Points: 5, Iterations: 20\n",
            "  (Proses ini memakan waktu 2-5 menit...)\n",
            "\n",
            "|   iter    |  target   | k_neig... | sampli... |     C     |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m0.5981497\u001b[39m | \u001b[39m5.6217808\u001b[39m | \u001b[39m0.9753571\u001b[39m | \u001b[39m73.199662\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m0.4617847\u001b[39m | \u001b[39m7.1906093\u001b[39m | \u001b[39m0.5780093\u001b[39m | \u001b[39m15.600296\u001b[39m |\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m0.6164978\u001b[39m | \u001b[35m3.4065852\u001b[39m | \u001b[35m0.9330880\u001b[39m | \u001b[35m60.111900\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m0.4159839\u001b[39m | \u001b[39m7.9565080\u001b[39m | \u001b[39m0.5102922\u001b[39m | \u001b[39m96.991015\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m0.5090554\u001b[39m | \u001b[39m8.8270984\u001b[39m | \u001b[39m0.6061695\u001b[39m | \u001b[39m18.183314\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m0.3995462\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m64.951164\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m0.5492277\u001b[39m | \u001b[39m6.3289111\u001b[39m | \u001b[39m0.7576964\u001b[39m | \u001b[39m78.697880\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m0.5987597\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m57.120563\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m0.5852671\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m69.449054\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m0.3780573\u001b[39m | \u001b[39m7.7698304\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m51.090196\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m0.5588354\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.7147026\u001b[39m | \u001b[39m27.515026\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m0.6164183\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m29.869368\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m0.4026225\u001b[39m | \u001b[39m5.4444084\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m34.678224\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m0.5986425\u001b[39m | \u001b[39m3.8912513\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m26.563539\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m0.4092366\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m75.276738\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m0.6070025\u001b[39m | \u001b[39m5.4696198\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m28.778517\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m0.5809549\u001b[39m | \u001b[39m6.7511434\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m70.563366\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m0.6078862\u001b[39m | \u001b[39m9.1306656\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m74.223155\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m0.5985309\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m64.372513\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m0.5827144\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m82.789736\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m0.5822917\u001b[39m | \u001b[39m5.5038924\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m85.715215\u001b[39m |\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m0.4433859\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m87.783797\u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m0.5927312\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m78.309194\u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m0.5812068\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.001    \u001b[39m |\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m0.5713735\u001b[39m | \u001b[39m8.5415261\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.001    \u001b[39m |\n",
            "=============================================================\n",
            "\n",
            "‚úÖ BEST PARAMETERS:\n",
            "  CV G-Mean: 0.6165\n",
            "  k_neighbors: 3\n",
            "  sampling_strategy: 0.9331  ‚Üê BERBEDA dari 0.8!\n",
            "  C: 60.1119\n",
            "\n",
            "üîÑ Training final model with best parameters...\n",
            "  ‚úì Samples after SMOTE: 1600\n",
            "\n",
            "‚úÖ FINAL TEST PERFORMANCE:\n",
            "  Accuracy: 0.6437\n",
            "  G-Mean: 0.6047\n",
            "  AUC-ROC: 0.6179\n",
            "\n",
            "==========================================================================================\n",
            " ‚úÖ LOGISTIC REGRESSION BO SELESAI!\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BAYESIAN OPTIMIZATION - ADABOOST (FIXED!)\n",
        "# ==============================================================================\n",
        "\n",
        "# PERBAIKAN: Define metrics di sini untuk menghindari NameError\n",
        "metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1-Score', 'G-Mean', 'AUC-ROC']\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" STEP 6B: BAYESIAN OPTIMIZATION - ADABOOST (FIXED!)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(\"\\nüîß PERBAIKAN KRITIS:\")\n",
        "print(\"  ‚úì sampling_strategy kini di-optimize (0.5 - 1.0)\")\n",
        "print(\"  ‚úì random_state=None untuk variasi synthetic data\")\n",
        "\n",
        "# Objective function for AdaBoost (FIXED!)\n",
        "def ada_objective(k_neighbors, sampling_strategy, n_estimators, learning_rate):\n",
        "    k = int(round(k_neighbors))\n",
        "    n_est = int(round(n_estimators))\n",
        "\n",
        "    # FIXED: sampling_strategy di-optimize!\n",
        "    smote = SMOTENC(\n",
        "        categorical_features=categorical_indices,\n",
        "        sampling_strategy=sampling_strategy,  # ‚Üê OPTIMIZE!\n",
        "        k_neighbors=k,\n",
        "        random_state=None  # ‚Üê None untuk variasi!\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "        model = AdaBoostClassifier(\n",
        "            n_estimators=n_est,\n",
        "            learning_rate=learning_rate,\n",
        "            random_state=42\n",
        "        )\n",
        "        score = cv_score_on_resampled(model, X_res, y_res, cv_splits=3)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "# Bayesian Optimization for AdaBoost\n",
        "ada_pbounds = {\n",
        "    'k_neighbors': (3, max_k_neighbors),\n",
        "    'sampling_strategy': (0.5, 1.0),  # ‚Üê BARU! Di-optimize!\n",
        "    'n_estimators': (10, 200),\n",
        "    'learning_rate': (0.01, 2.0)\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Parameter Bounds:\")\n",
        "print(f\"  k_neighbors: (3, {max_k_neighbors})\")\n",
        "print(f\"  sampling_strategy: (0.5, 1.0)  ‚Üê BARU!\")\n",
        "print(f\"  n_estimators: (10, 200)\")\n",
        "print(f\"  learning_rate: (0.01, 2.0)\")\n",
        "\n",
        "print(f\"\\nüöÄ Starting Bayesian Optimization untuk AdaBoost...\")\n",
        "print(f\"  Init Points: 5, Iterations: 20\")\n",
        "print(f\"  (Proses ini memakan waktu 3-7 menit...)\\n\")\n",
        "\n",
        "ada_optimizer = BayesianOptimization(\n",
        "    f=ada_objective,\n",
        "    pbounds=ada_pbounds,\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "ada_optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "ada_best_params = ada_optimizer.max['params']\n",
        "ada_best_score = ada_optimizer.max['target']\n",
        "\n",
        "print(f\"\\n‚úÖ BEST PARAMETERS FOR ADABOOST:\")\n",
        "print(f\"  CV G-Mean: {ada_best_score:.4f}\")\n",
        "print(f\"  k_neighbors: {int(round(ada_best_params['k_neighbors']))}\")\n",
        "print(f\"  sampling_strategy: {ada_best_params['sampling_strategy']:.4f}  ‚Üê BERBEDA dari 0.8!\")\n",
        "print(f\"  n_estimators: {int(round(ada_best_params['n_estimators']))}\")\n",
        "print(f\"  learning_rate: {ada_best_params['learning_rate']:.4f}\")\n",
        "\n",
        "# Train final AdaBoost model\n",
        "print(f\"\\nüîÑ Training final AdaBoost model with best parameters...\")\n",
        "smote_ada_best = SMOTENC(\n",
        "    categorical_features=categorical_indices,\n",
        "    sampling_strategy=ada_best_params['sampling_strategy'],\n",
        "    k_neighbors=int(round(ada_best_params['k_neighbors'])),\n",
        "    random_state=42\n",
        ")\n",
        "X_train_ada_bo, y_train_ada_bo = smote_ada_best.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"  ‚úì Samples after SMOTE: {len(y_train_ada_bo)}\")\n",
        "\n",
        "ada_final = AdaBoostClassifier(\n",
        "    n_estimators=int(round(ada_best_params['n_estimators'])),\n",
        "    learning_rate=ada_best_params['learning_rate'],\n",
        "    random_state=42\n",
        ")\n",
        "ada_final.fit(X_train_ada_bo, y_train_ada_bo)\n",
        "\n",
        "y_pred_ada_bo = ada_final.predict(X_test)\n",
        "y_pred_proba_ada_bo = ada_final.predict_proba(X_test)[:, 1]\n",
        "\n",
        "result_ada_bo = evaluate_model(y_test, y_pred_ada_bo, y_pred_proba_ada_bo, \"AdaBoost (Fixed Sampling + BO)\")\n",
        "\n",
        "print(f\"\\n‚úÖ FINAL TEST PERFORMANCE:\")\n",
        "print(f\"  Accuracy: {result_ada_bo['Accuracy']:.4f}\")\n",
        "print(f\"  G-Mean: {result_ada_bo['G-Mean']:.4f}\")\n",
        "print(f\"  AUC-ROC: {result_ada_bo['AUC-ROC']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" ‚úÖ ADABOOST BO SELESAI!\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Comparison with Default\n",
        "print(f\"\\nüìä ADABOOST - Perbandingan Detail:\\n\")\n",
        "print(f\"{'Metric':<20} {'SMOTE Default':>15} {'BO (Fixed)':>15} {'Difference':>15} {'Status':>20}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "ada_default = smote_default_results[1]  # AdaBoost dari SMOTE Default\n",
        "\n",
        "for metric in metrics:\n",
        "    default_val = ada_default[metric]\n",
        "    bo_val = result_ada_bo[metric]\n",
        "    diff = bo_val - default_val\n",
        "\n",
        "    if abs(diff) < 0.001:\n",
        "        status = \"‚ö†Ô∏è IDENTIK\"\n",
        "    elif abs(diff) < 0.01:\n",
        "        status = \"‚úì Sedikit Berbeda\"\n",
        "    else:\n",
        "        status = \"‚úÖ BERBEDA!\" if diff > 0 else \"‚úÖ BERBEDA\"\n",
        "\n",
        "    print(f\"{metric:<20} {default_val:>15.4f} {bo_val:>15.4f} {diff:>+15.4f} {status:>20}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE-75c0pplhK",
        "outputId": "2de8a732-5b1e-42a1-b4d4-47a573d010a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " STEP 6B: BAYESIAN OPTIMIZATION - ADABOOST (FIXED!)\n",
            "==========================================================================================\n",
            "\n",
            "üîß PERBAIKAN KRITIS:\n",
            "  ‚úì sampling_strategy kini di-optimize (0.5 - 1.0)\n",
            "  ‚úì random_state=None untuk variasi synthetic data\n",
            "\n",
            "‚öôÔ∏è Parameter Bounds:\n",
            "  k_neighbors: (3, 10)\n",
            "  sampling_strategy: (0.5, 1.0)  ‚Üê BARU!\n",
            "  n_estimators: (10, 200)\n",
            "  learning_rate: (0.01, 2.0)\n",
            "\n",
            "üöÄ Starting Bayesian Optimization untuk AdaBoost...\n",
            "  Init Points: 5, Iterations: 20\n",
            "  (Proses ini memakan waktu 3-7 menit...)\n",
            "\n",
            "|   iter    |  target   | k_neig... | sampli... | n_esti... | learni... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m0.7070306\u001b[39m | \u001b[39m5.6217808\u001b[39m | \u001b[39m0.9753571\u001b[39m | \u001b[39m149.07884\u001b[39m | \u001b[39m1.2013303\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m0.6728611\u001b[39m | \u001b[39m4.0921304\u001b[39m | \u001b[39m0.5779972\u001b[39m | \u001b[39m21.035886\u001b[39m | \u001b[39m1.7336905\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m0.4865124\u001b[39m | \u001b[39m7.2078050\u001b[39m | \u001b[39m0.8540362\u001b[39m | \u001b[39m13.911053\u001b[39m | \u001b[39m1.9401206\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m0.5062674\u001b[39m | \u001b[39m8.8270984\u001b[39m | \u001b[39m0.6061695\u001b[39m | \u001b[39m44.546743\u001b[39m | \u001b[39m0.3749749\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m0.6810531\u001b[39m | \u001b[39m5.1296957\u001b[39m | \u001b[39m0.7623782\u001b[39m | \u001b[39m92.069553\u001b[39m | \u001b[39m0.5895459\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m0.6519107\u001b[39m | \u001b[39m8.4962317\u001b[39m | \u001b[39m0.5760739\u001b[39m | \u001b[39m83.230593\u001b[39m | \u001b[39m0.9563618\u001b[39m |\n",
            "| \u001b[35m7        \u001b[39m | \u001b[35m0.7174962\u001b[39m | \u001b[35m6.1550666\u001b[39m | \u001b[35m0.8244336\u001b[39m | \u001b[35m39.191148\u001b[39m | \u001b[35m1.5210060\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m0.5683341\u001b[39m | \u001b[39m6.2167431\u001b[39m | \u001b[39m0.6011818\u001b[39m | \u001b[39m27.201138\u001b[39m | \u001b[39m0.6369281\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m0.5096423\u001b[39m | \u001b[39m6.6330435\u001b[39m | \u001b[39m0.5605124\u001b[39m | \u001b[39m24.895552\u001b[39m | \u001b[39m0.2105170\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m0.6489748\u001b[39m | \u001b[39m5.2126002\u001b[39m | \u001b[39m0.9012047\u001b[39m | \u001b[39m37.368540\u001b[39m | \u001b[39m1.9233390\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m0.6531782\u001b[39m | \u001b[39m8.1958524\u001b[39m | \u001b[39m0.6849255\u001b[39m | \u001b[39m38.426122\u001b[39m | \u001b[39m0.8333793\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m0.5130535\u001b[39m | \u001b[39m4.5498242\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m40.148313\u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m0.4303495\u001b[39m | \u001b[39m6.8735100\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m38.359601\u001b[39m | \u001b[39m2.0      \u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m0.7167172\u001b[39m | \u001b[39m6.8036874\u001b[39m | \u001b[39m0.9989107\u001b[39m | \u001b[39m95.440804\u001b[39m | \u001b[39m1.0883625\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m0.6979501\u001b[39m | \u001b[39m6.0513779\u001b[39m | \u001b[39m0.7584264\u001b[39m | \u001b[39m49.011000\u001b[39m | \u001b[39m1.5615535\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m0.6964246\u001b[39m | \u001b[39m7.1851749\u001b[39m | \u001b[39m0.7562144\u001b[39m | \u001b[39m149.94498\u001b[39m | \u001b[39m0.9045137\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m0.7066190\u001b[39m | \u001b[39m6.7516853\u001b[39m | \u001b[39m0.8045266\u001b[39m | \u001b[39m81.311049\u001b[39m | \u001b[39m1.3798829\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m0.5879819\u001b[39m | \u001b[39m8.0368428\u001b[39m | \u001b[39m0.6742352\u001b[39m | \u001b[39m18.495720\u001b[39m | \u001b[39m0.7124826\u001b[39m |\n",
            "| \u001b[35m19       \u001b[39m | \u001b[35m0.7196356\u001b[39m | \u001b[35m4.7687535\u001b[39m | \u001b[35m0.7815009\u001b[39m | \u001b[35m152.71910\u001b[39m | \u001b[35m1.3811145\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m0.7176580\u001b[39m | \u001b[39m6.7073651\u001b[39m | \u001b[39m0.8461195\u001b[39m | \u001b[39m81.311461\u001b[39m | \u001b[39m1.4432447\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m0.7168023\u001b[39m | \u001b[39m6.0790311\u001b[39m | \u001b[39m0.9805422\u001b[39m | \u001b[39m81.131771\u001b[39m | \u001b[39m1.4137792\u001b[39m |\n",
            "| \u001b[35m22       \u001b[39m | \u001b[35m0.7220315\u001b[39m | \u001b[35m6.2170405\u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m81.862189\u001b[39m | \u001b[35m1.8424469\u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m0.7095030\u001b[39m | \u001b[39m5.9235289\u001b[39m | \u001b[39m0.7188789\u001b[39m | \u001b[39m40.016877\u001b[39m | \u001b[39m1.6327597\u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m0.7090659\u001b[39m | \u001b[39m5.2002900\u001b[39m | \u001b[39m0.8155286\u001b[39m | \u001b[39m39.238551\u001b[39m | \u001b[39m1.7622470\u001b[39m |\n",
            "| \u001b[35m25       \u001b[39m | \u001b[35m0.7328191\u001b[39m | \u001b[35m5.8445388\u001b[39m | \u001b[35m0.8546332\u001b[39m | \u001b[35m150.37655\u001b[39m | \u001b[35m1.1641069\u001b[39m |\n",
            "=========================================================================\n",
            "\n",
            "‚úÖ BEST PARAMETERS FOR ADABOOST:\n",
            "  CV G-Mean: 0.7328\n",
            "  k_neighbors: 6\n",
            "  sampling_strategy: 0.8546  ‚Üê BERBEDA dari 0.8!\n",
            "  n_estimators: 150\n",
            "  learning_rate: 1.1641\n",
            "\n",
            "üîÑ Training final AdaBoost model with best parameters...\n",
            "  ‚úì Samples after SMOTE: 1535\n",
            "\n",
            "‚úÖ FINAL TEST PERFORMANCE:\n",
            "  Accuracy: 0.8161\n",
            "  G-Mean: 0.7448\n",
            "  AUC-ROC: 0.8089\n",
            "\n",
            "==========================================================================================\n",
            " ‚úÖ ADABOOST BO SELESAI!\n",
            "==========================================================================================\n",
            "\n",
            "üìä ADABOOST - Perbandingan Detail:\n",
            "\n",
            "Metric                 SMOTE Default      BO (Fixed)      Difference               Status\n",
            "------------------------------------------------------------------------------------------\n",
            "Accuracy                      0.8138          0.8161         +0.0023    ‚úì Sedikit Berbeda\n",
            "Sensitivity                   0.6375          0.6500         +0.0125           ‚úÖ BERBEDA!\n",
            "Specificity                   0.8535          0.8535         +0.0000           ‚ö†Ô∏è IDENTIK\n",
            "Precision                     0.4951          0.5000         +0.0049    ‚úì Sedikit Berbeda\n",
            "F1-Score                      0.5574          0.5652         +0.0078    ‚úì Sedikit Berbeda\n",
            "G-Mean                        0.7376          0.7448         +0.0072    ‚úì Sedikit Berbeda\n",
            "AUC-ROC                       0.8136          0.8089         -0.0047    ‚úì Sedikit Berbeda\n",
            "\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics (untuk menghindari NameError)\n",
        "metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1-Score', 'G-Mean', 'AUC-ROC']\n",
        "# ==============================================================================\n",
        "# BAYESIAN OPTIMIZATION - XGBOOST (FIXED!)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" STEP 6C: BAYESIAN OPTIMIZATION - XGBOOST (FIXED!)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(\"\\nüîß PERBAIKAN KRITIS:\")\n",
        "print(\"  ‚úì sampling_strategy kini di-optimize (0.5 - 1.0)\")\n",
        "print(\"  ‚úì random_state=None untuk variasi synthetic data\")\n",
        "\n",
        "# Objective function for XGBoost (FIXED!)\n",
        "def xgb_objective(k_neighbors, sampling_strategy, n_estimators, max_depth, learning_rate):\n",
        "    k = int(round(k_neighbors))\n",
        "    n_est = int(round(n_estimators))\n",
        "    depth = int(round(max_depth))\n",
        "\n",
        "    # FIXED: sampling_strategy di-optimize!\n",
        "    smote = SMOTENC(\n",
        "        categorical_features=categorical_indices,\n",
        "        sampling_strategy=sampling_strategy,  # ‚Üê OPTIMIZE!\n",
        "        k_neighbors=k,\n",
        "        random_state=None  # ‚Üê None untuk variasi!\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=n_est,\n",
        "            max_depth=depth,\n",
        "            learning_rate=learning_rate,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "        score = cv_score_on_resampled(model, X_res, y_res, cv_splits=3)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "# Bayesian Optimization for XGBoost\n",
        "xgb_pbounds = {\n",
        "    'k_neighbors': (3, max_k_neighbors),\n",
        "    'sampling_strategy': (0.5, 1.0),  # ‚Üê BARU! Di-optimize!\n",
        "    'n_estimators': (50, 300),\n",
        "    'max_depth': (3, 10),\n",
        "    'learning_rate': (0.01, 0.3)\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Parameter Bounds:\")\n",
        "print(f\"  k_neighbors: (3, {max_k_neighbors})\")\n",
        "print(f\"  sampling_strategy: (0.5, 1.0)  ‚Üê BARU!\")\n",
        "print(f\"  n_estimators: (50, 300)\")\n",
        "print(f\"  max_depth: (3, 10)\")\n",
        "print(f\"  learning_rate: (0.01, 0.3)\")\n",
        "\n",
        "print(f\"\\nüöÄ Starting Bayesian Optimization untuk XGBoost...\")\n",
        "print(f\"  Init Points: 5, Iterations: 20\")\n",
        "print(f\"  (Proses ini memakan waktu 5-10 menit...)\\n\")\n",
        "\n",
        "xgb_optimizer = BayesianOptimization(\n",
        "    f=xgb_objective,\n",
        "    pbounds=xgb_pbounds,\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "xgb_optimizer.maximize(init_points=5, n_iter=20)\n",
        "\n",
        "xgb_best_params = xgb_optimizer.max['params']\n",
        "xgb_best_score = xgb_optimizer.max['target']\n",
        "\n",
        "print(f\"\\n‚úÖ BEST PARAMETERS FOR XGBOOST:\")\n",
        "print(f\"  CV G-Mean: {xgb_best_score:.4f}\")\n",
        "print(f\"  k_neighbors: {int(round(xgb_best_params['k_neighbors']))}\")\n",
        "print(f\"  sampling_strategy: {xgb_best_params['sampling_strategy']:.4f}  ‚Üê BERBEDA dari 0.8!\")\n",
        "print(f\"  n_estimators: {int(round(xgb_best_params['n_estimators']))}\")\n",
        "print(f\"  max_depth: {int(round(xgb_best_params['max_depth']))}\")\n",
        "print(f\"  learning_rate: {xgb_best_params['learning_rate']:.4f}\")\n",
        "\n",
        "# Train final XGBoost model\n",
        "print(f\"\\nüîÑ Training final XGBoost model with best parameters...\")\n",
        "smote_xgb_best = SMOTENC(\n",
        "    categorical_features=categorical_indices,\n",
        "    sampling_strategy=xgb_best_params['sampling_strategy'],\n",
        "    k_neighbors=int(round(xgb_best_params['k_neighbors'])),\n",
        "    random_state=42\n",
        ")\n",
        "X_train_xgb_bo, y_train_xgb_bo = smote_xgb_best.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"  ‚úì Samples after SMOTE: {len(y_train_xgb_bo)}\")\n",
        "\n",
        "xgb_final = XGBClassifier(\n",
        "    n_estimators=int(round(xgb_best_params['n_estimators'])),\n",
        "    max_depth=int(round(xgb_best_params['max_depth'])),\n",
        "    learning_rate=xgb_best_params['learning_rate'],\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "xgb_final.fit(X_train_xgb_bo, y_train_xgb_bo)\n",
        "\n",
        "y_pred_xgb_bo = xgb_final.predict(X_test)\n",
        "y_pred_proba_xgb_bo = xgb_final.predict_proba(X_test)[:, 1]\n",
        "\n",
        "result_xgb_bo = evaluate_model(y_test, y_pred_xgb_bo, y_pred_proba_xgb_bo, \"XGBoost (Fixed Sampling + BO)\")\n",
        "\n",
        "print(f\"\\n‚úÖ FINAL TEST PERFORMANCE:\")\n",
        "print(f\"  Accuracy: {result_xgb_bo['Accuracy']:.4f}\")\n",
        "print(f\"  G-Mean: {result_xgb_bo['G-Mean']:.4f}\")\n",
        "print(f\"  AUC-ROC: {result_xgb_bo['AUC-ROC']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" ‚úÖ XGBOOST BO SELESAI!\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Comparison with Default\n",
        "print(f\"\\nüìä XGBOOST - Perbandingan Detail:\\n\")\n",
        "print(f\"{'Metric':<20} {'SMOTE Default':>15} {'BO (Fixed)':>15} {'Difference':>15} {'Status':>20}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "xgb_default = smote_default_results[2]  # XGBoost dari SMOTE Default\n",
        "\n",
        "for metric in metrics:\n",
        "    default_val = xgb_default[metric]\n",
        "    bo_val = result_xgb_bo[metric]\n",
        "    diff = bo_val - default_val\n",
        "\n",
        "    if abs(diff) < 0.001:\n",
        "        status = \"‚ö†Ô∏è IDENTIK\"\n",
        "    elif abs(diff) < 0.01:\n",
        "        status = \"‚úì Sedikit Berbeda\"\n",
        "    else:\n",
        "        status = \"‚úÖ BERBEDA!\" if diff > 0 else \"‚úÖ BERBEDA\"\n",
        "\n",
        "    print(f\"{metric:<20} {default_val:>15.4f} {bo_val:>15.4f} {diff:>+15.4f} {status:>20}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2lxUaFwp1JH",
        "outputId": "19dc2839-891f-4272-a4db-b07147bcbcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " STEP 6C: BAYESIAN OPTIMIZATION - XGBOOST (FIXED!)\n",
            "==========================================================================================\n",
            "\n",
            "üîß PERBAIKAN KRITIS:\n",
            "  ‚úì sampling_strategy kini di-optimize (0.5 - 1.0)\n",
            "  ‚úì random_state=None untuk variasi synthetic data\n",
            "\n",
            "‚öôÔ∏è Parameter Bounds:\n",
            "  k_neighbors: (3, 10)\n",
            "  sampling_strategy: (0.5, 1.0)  ‚Üê BARU!\n",
            "  n_estimators: (50, 300)\n",
            "  max_depth: (3, 10)\n",
            "  learning_rate: (0.01, 0.3)\n",
            "\n",
            "üöÄ Starting Bayesian Optimization untuk XGBoost...\n",
            "  Init Points: 5, Iterations: 20\n",
            "  (Proses ini memakan waktu 5-10 menit...)\n",
            "\n",
            "|   iter    |  target   | k_neig... | sampli... | n_esti... | max_depth | learni... |\n",
            "-------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m0.8005485\u001b[39m | \u001b[39m5.6217808\u001b[39m | \u001b[39m0.9753571\u001b[39m | \u001b[39m232.99848\u001b[39m | \u001b[39m7.1906093\u001b[39m | \u001b[39m0.0552454\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m0.7604837\u001b[39m | \u001b[39m4.0919616\u001b[39m | \u001b[39m0.5290418\u001b[39m | \u001b[39m266.54403\u001b[39m | \u001b[39m7.2078050\u001b[39m | \u001b[39m0.2153410\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m0.7816779\u001b[39m | \u001b[39m3.1440914\u001b[39m | \u001b[39m0.9849549\u001b[39m | \u001b[39m258.11066\u001b[39m | \u001b[39m4.4863737\u001b[39m | \u001b[39m0.0627292\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m0.7675222\u001b[39m | \u001b[39m4.2838315\u001b[39m | \u001b[39m0.6521211\u001b[39m | \u001b[39m181.18910\u001b[39m | \u001b[39m6.0236151\u001b[39m | \u001b[39m0.0944564\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m0.7602591\u001b[39m | \u001b[39m7.2829702\u001b[39m | \u001b[39m0.5697469\u001b[39m | \u001b[39m123.03616\u001b[39m | \u001b[39m5.5645329\u001b[39m | \u001b[39m0.1422602\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m0.7534082\u001b[39m | \u001b[39m7.2528139\u001b[39m | \u001b[39m0.5026695\u001b[39m | \u001b[39m246.36125\u001b[39m | \u001b[39m7.4214813\u001b[39m | \u001b[39m0.1407099\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m0.7912356\u001b[39m | \u001b[39m5.7677349\u001b[39m | \u001b[39m0.7953603\u001b[39m | \u001b[39m121.25844\u001b[39m | \u001b[39m6.2167431\u001b[39m | \u001b[39m0.0686854\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m0.7464372\u001b[39m | \u001b[39m6.1910888\u001b[39m | \u001b[39m0.8834803\u001b[39m | \u001b[39m179.75155\u001b[39m | \u001b[39m3.8471741\u001b[39m | \u001b[39m0.0327353\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m0.7855546\u001b[39m | \u001b[39m3.7497110\u001b[39m | \u001b[39m0.7626461\u001b[39m | \u001b[39m89.885544\u001b[39m | \u001b[39m8.7826512\u001b[39m | \u001b[39m0.2255948\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m0.7907690\u001b[39m | \u001b[39m9.6431278\u001b[39m | \u001b[39m0.9901306\u001b[39m | \u001b[39m202.23907\u001b[39m | \u001b[39m6.4862484\u001b[39m | \u001b[39m0.1071260\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m0.7374501\u001b[39m | \u001b[39m9.7625063\u001b[39m | \u001b[39m0.5433554\u001b[39m | \u001b[39m219.06316\u001b[39m | \u001b[39m3.3171881\u001b[39m | \u001b[39m0.1307302\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m0.7879398\u001b[39m | \u001b[39m9.4944274\u001b[39m | \u001b[39m0.9597814\u001b[39m | \u001b[39m216.30211\u001b[39m | \u001b[39m7.8965029\u001b[39m | \u001b[39m0.2676775\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m0.7423111\u001b[39m | \u001b[39m5.9669261\u001b[39m | \u001b[39m0.8406887\u001b[39m | \u001b[39m120.97289\u001b[39m | \u001b[39m5.8742177\u001b[39m | \u001b[39m0.0390451\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m0.7386427\u001b[39m | \u001b[39m8.0288253\u001b[39m | \u001b[39m0.5443854\u001b[39m | \u001b[39m219.63888\u001b[39m | \u001b[39m7.2707544\u001b[39m | \u001b[39m0.2442082\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m0.7695391\u001b[39m | \u001b[39m7.6120251\u001b[39m | \u001b[39m0.6224369\u001b[39m | \u001b[39m271.27746\u001b[39m | \u001b[39m9.4986932\u001b[39m | \u001b[39m0.2754822\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m0.7936838\u001b[39m | \u001b[39m6.3317738\u001b[39m | \u001b[39m0.9049865\u001b[39m | \u001b[39m71.326075\u001b[39m | \u001b[39m6.3254006\u001b[39m | \u001b[39m0.2940868\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m0.7503441\u001b[39m | \u001b[39m4.6763494\u001b[39m | \u001b[39m0.6676905\u001b[39m | \u001b[39m154.19592\u001b[39m | \u001b[39m9.2443947\u001b[39m | \u001b[39m0.0196309\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m0.7521677\u001b[39m | \u001b[39m8.8448063\u001b[39m | \u001b[39m0.5475775\u001b[39m | \u001b[39m250.14216\u001b[39m | \u001b[39m4.2218279\u001b[39m | \u001b[39m0.0503142\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m0.7722656\u001b[39m | \u001b[39m7.4137453\u001b[39m | \u001b[39m0.6734804\u001b[39m | \u001b[39m224.69898\u001b[39m | \u001b[39m7.5956285\u001b[39m | \u001b[39m0.0850988\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m0.7449819\u001b[39m | \u001b[39m9.2592843\u001b[39m | \u001b[39m0.6766641\u001b[39m | \u001b[39m118.27389\u001b[39m | \u001b[39m3.6549003\u001b[39m | \u001b[39m0.1399206\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m0.7950213\u001b[39m | \u001b[39m4.9971873\u001b[39m | \u001b[39m0.8971083\u001b[39m | \u001b[39m57.081241\u001b[39m | \u001b[39m8.7574303\u001b[39m | \u001b[39m0.1654754\u001b[39m |\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m0.7505160\u001b[39m | \u001b[39m6.6045694\u001b[39m | \u001b[39m0.6271629\u001b[39m | \u001b[39m156.52473\u001b[39m | \u001b[39m4.9455220\u001b[39m | \u001b[39m0.1113326\u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m0.7950131\u001b[39m | \u001b[39m5.3460021\u001b[39m | \u001b[39m0.8218893\u001b[39m | \u001b[39m162.31740\u001b[39m | \u001b[39m4.1885247\u001b[39m | \u001b[39m0.1001558\u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m0.7380768\u001b[39m | \u001b[39m9.1064545\u001b[39m | \u001b[39m0.6157577\u001b[39m | \u001b[39m157.68951\u001b[39m | \u001b[39m7.7311617\u001b[39m | \u001b[39m0.0712479\u001b[39m |\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m0.7608873\u001b[39m | \u001b[39m8.2294184\u001b[39m | \u001b[39m0.7857678\u001b[39m | \u001b[39m299.79097\u001b[39m | \u001b[39m5.4501494\u001b[39m | \u001b[39m0.1564081\u001b[39m |\n",
            "=====================================================================================\n",
            "\n",
            "‚úÖ BEST PARAMETERS FOR XGBOOST:\n",
            "  CV G-Mean: 0.8005\n",
            "  k_neighbors: 6\n",
            "  sampling_strategy: 0.9754  ‚Üê BERBEDA dari 0.8!\n",
            "  n_estimators: 233\n",
            "  max_depth: 7\n",
            "  learning_rate: 0.0552\n",
            "\n",
            "üîÑ Training final XGBoost model with best parameters...\n",
            "  ‚úì Samples after SMOTE: 1635\n",
            "\n",
            "‚úÖ FINAL TEST PERFORMANCE:\n",
            "  Accuracy: 0.7356\n",
            "  G-Mean: 0.6143\n",
            "  AUC-ROC: 0.6986\n",
            "\n",
            "==========================================================================================\n",
            " ‚úÖ XGBOOST BO SELESAI!\n",
            "==========================================================================================\n",
            "\n",
            "üìä XGBOOST - Perbandingan Detail:\n",
            "\n",
            "Metric                 SMOTE Default      BO (Fixed)      Difference               Status\n",
            "------------------------------------------------------------------------------------------\n",
            "Accuracy                      0.7540          0.7356         -0.0184            ‚úÖ BERBEDA\n",
            "Sensitivity                   0.4750          0.4750         +0.0000           ‚ö†Ô∏è IDENTIK\n",
            "Specificity                   0.8169          0.7944         -0.0225            ‚úÖ BERBEDA\n",
            "Precision                     0.3689          0.3423         -0.0266            ‚úÖ BERBEDA\n",
            "F1-Score                      0.4153          0.3979         -0.0174            ‚úÖ BERBEDA\n",
            "G-Mean                        0.6229          0.6143         -0.0087    ‚úì Sedikit Berbeda\n",
            "AUC-ROC                       0.7238          0.6986         -0.0253            ‚úÖ BERBEDA\n",
            "\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# COMPREHENSIVE COMPARISON - ALL RESULTS\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" STEP 7: COMPREHENSIVE COMPARISON\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Collect all BO results\n",
        "fixed_sampling_bo_results = [result_lr_bo, result_ada_bo, result_xgb_bo]\n",
        "\n",
        "# Create comprehensive dataframe\n",
        "all_results = baseline_results + smote_default_results + fixed_sampling_bo_results\n",
        "df_all = pd.DataFrame(all_results)\n",
        "\n",
        "# Display metrics to show\n",
        "display_metrics = ['Model', 'Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1-Score', 'G-Mean', 'AUC-ROC']\n",
        "\n",
        "# ==============================================================================\n",
        "# 1Ô∏è‚É£ BASELINE (No SMOTE)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" 1Ô∏è‚É£ BASELINE (No SMOTE)\")\n",
        "print(\"=\"*90)\n",
        "print(\"\\n\" + df_all.iloc[0:3][display_metrics].to_string(index=False))\n",
        "\n",
        "# ==============================================================================\n",
        "# 2Ô∏è‚É£ SMOTE-NC DEFAULT\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" 2Ô∏è‚É£ SMOTE-NC DEFAULT (sampling_strategy=0.8, k_neighbors=5)\")\n",
        "print(\"=\"*90)\n",
        "print(\"\\n\" + df_all.iloc[3:6][display_metrics].to_string(index=False))\n",
        "\n",
        "# ==============================================================================\n",
        "# 3Ô∏è‚É£ FIXED SAMPLING + BAYESIAN OPTIMIZATION\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" 3Ô∏è‚É£ FIXED SAMPLING + BAYESIAN OPTIMIZATION (v8.2 FIXED)\")\n",
        "print(\"=\"*90)\n",
        "print(\"\\n\" + df_all.iloc[6:9][display_metrics].to_string(index=False))\n",
        "\n",
        "# ==============================================================================\n",
        "# üìä PERBANDINGAN LENGKAP\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" üìä PERBANDINGAN LENGKAP: Semua Eksperimen\")\n",
        "print(\"=\"*90)\n",
        "print(\"\\n\" + df_all[display_metrics].to_string(index=False))\n",
        "\n",
        "# ==============================================================================\n",
        "# üèÜ BEST MODEL\n",
        "# ==============================================================================\n",
        "best_idx = df_all['G-Mean'].idxmax()\n",
        "best_model = df_all.loc[best_idx]\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(f\" üèÜ MODEL TERBAIK KESELURUHAN: {best_model['Model']}\")\n",
        "print(\"=\"*90)\n",
        "print(f\"   G-Mean Score: {best_model['G-Mean']:.4f}\")\n",
        "print(f\"   Sensitivity : {best_model['Sensitivity']:.4f}\")\n",
        "print(f\"   Specificity : {best_model['Specificity']:.4f}\")\n",
        "print(f\"   Accuracy    : {best_model['Accuracy']:.4f}\")\n",
        "print(f\"   AUC-ROC     : {best_model['AUC-ROC']:.4f}\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# ==============================================================================\n",
        "# üìà IMPROVEMENT ANALYSIS\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" üìà IMPROVEMENT ANALYSIS: Default vs BO (FIXED)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "models_comparison = [\n",
        "    ('Logistic Regression', 3, 6),\n",
        "    ('AdaBoost', 4, 7),\n",
        "    ('XGBoost', 5, 8)\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Model':<25} {'Default G-Mean':>15} {'BO G-Mean':>15} {'Improvement':>15} {'Status':>20}\")\n",
        "print(\"-\" * 95)\n",
        "\n",
        "for model_name, default_idx, bo_idx in models_comparison:\n",
        "    default_gmean = df_all.iloc[default_idx]['G-Mean']\n",
        "    bo_gmean = df_all.iloc[bo_idx]['G-Mean']\n",
        "    improvement = ((bo_gmean - default_gmean) / default_gmean) * 100\n",
        "\n",
        "    if improvement > 1:\n",
        "        status = \"‚úÖ IMPROVEMENT!\"\n",
        "    elif improvement > 0:\n",
        "        status = \"‚úì Slight Improvement\"\n",
        "    elif improvement < -1:\n",
        "        status = \"‚ö†Ô∏è DEGRADATION\"\n",
        "    else:\n",
        "        status = \"‚âà Similar\"\n",
        "\n",
        "    print(f\"{model_name:<25} {default_gmean:>15.4f} {bo_gmean:>15.4f} {improvement:>+14.2f}% {status:>20}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "\n",
        "# ==============================================================================\n",
        "# üîç VERIFICATION: Check for Identical Results\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" üîç VERIFICATION: Cek Hasil Identik\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(\"\\nüìã Detailed Comparison untuk setiap model:\\n\")\n",
        "\n",
        "for model_name, default_idx, bo_idx in models_comparison:\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    default_row = df_all.iloc[default_idx]\n",
        "    bo_row = df_all.iloc[bo_idx]\n",
        "\n",
        "    identical_count = 0\n",
        "\n",
        "    for metric in ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1-Score', 'G-Mean', 'AUC-ROC']:\n",
        "        default_val = default_row[metric]\n",
        "        bo_val = bo_row[metric]\n",
        "        diff = abs(default_val - bo_val)\n",
        "\n",
        "        if diff == 0:\n",
        "            status = \"‚ö†Ô∏è IDENTIK\"\n",
        "            identical_count += 1\n",
        "        elif diff < 0.001:\n",
        "            status = \"‚ö†Ô∏è HAMPIR IDENTIK\"\n",
        "        elif diff < 0.01:\n",
        "            status = \"‚úì Perbedaan Kecil\"\n",
        "        else:\n",
        "            status = \"‚úÖ Berbeda Signifikan\"\n",
        "\n",
        "        print(f\"  {metric:15s}: Default={default_val:.4f}, BO={bo_val:.4f}, Diff={diff:.4f} {status}\")\n",
        "\n",
        "    print(f\"\\n  TOTAL METRIK IDENTIK: {identical_count}/7\")\n",
        "\n",
        "    if identical_count >= 5:\n",
        "        print(\"  üö® PERINGATAN: Terlalu banyak metrik identik!\")\n",
        "    elif identical_count == 0:\n",
        "        print(\"  ‚úÖ BAGUS: Tidak ada metrik yang identik - BO bekerja dengan baik!\")\n",
        "    else:\n",
        "        print(\"  ‚úì OK: Beberapa perbedaan terdeteksi\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "\n",
        "# ==============================================================================\n",
        "# üíæ SAVE RESULTS\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" üíæ SAVE RESULTS\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Save to CSV\n",
        "import os\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "\n",
        "df_all.to_csv('results/all_results_comprehensive.csv', index=False)\n",
        "print(\"\\n‚úÖ Results saved: results/all_results_comprehensive.csv\")\n",
        "\n",
        "# Save individual results\n",
        "df_baseline = pd.DataFrame(baseline_results)\n",
        "df_baseline.to_csv('results/baseline_results.csv', index=False)\n",
        "print(\"‚úÖ Results saved: results/baseline_results.csv\")\n",
        "\n",
        "df_smote_default = pd.DataFrame(smote_default_results)\n",
        "df_smote_default.to_csv('results/smote_nc_default_results.csv', index=False)\n",
        "print(\"‚úÖ Results saved: results/smote_nc_default_results.csv\")\n",
        "\n",
        "df_fixed_bo = pd.DataFrame(fixed_sampling_bo_results)\n",
        "df_fixed_bo.to_csv('results/fixed_sampling_optimized_results.csv', index=False)\n",
        "print(\"‚úÖ Results saved: results/fixed_sampling_optimized_results.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" ‚úÖ‚úÖ‚úÖ ALL EXPERIMENTS COMPLETED! ‚úÖ‚úÖ‚úÖ\")\n",
        "print(\"=\"*90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I-46aK0qdFx",
        "outputId": "7afc29a9-cdc7-4cca-e58c-8d3a82f9a328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " STEP 7: COMPREHENSIVE COMPARISON\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            " 1Ô∏è‚É£ BASELINE (No SMOTE)\n",
            "==========================================================================================\n",
            "\n",
            "                         Model  Accuracy  Sensitivity  Specificity  Precision  F1-Score   G-Mean  AUC-ROC\n",
            "Logistic Regression (Baseline)  0.816092       0.0000     1.000000   0.000000  0.000000 0.000000 0.614366\n",
            "           AdaBoost (Baseline)  0.827586       0.3125     0.943662   0.555556  0.400000 0.543042 0.791532\n",
            "            XGBoost (Baseline)  0.786207       0.3375     0.887324   0.402985  0.367347 0.547240 0.713556\n",
            "\n",
            "==========================================================================================\n",
            " 2Ô∏è‚É£ SMOTE-NC DEFAULT (sampling_strategy=0.8, k_neighbors=5)\n",
            "==========================================================================================\n",
            "\n",
            "                                 Model  Accuracy  Sensitivity  Specificity  Precision  F1-Score   G-Mean  AUC-ROC\n",
            "Logistic Regression (SMOTE-NC Default)  0.701149       0.4500     0.757746   0.295082  0.356436 0.583940 0.623592\n",
            "           AdaBoost (SMOTE-NC Default)  0.813793       0.6375     0.853521   0.495146  0.557377 0.737645 0.813592\n",
            "            XGBoost (SMOTE-NC Default)  0.754023       0.4750     0.816901   0.368932  0.415301 0.622919 0.723838\n",
            "\n",
            "==========================================================================================\n",
            " 3Ô∏è‚É£ FIXED SAMPLING + BAYESIAN OPTIMIZATION (v8.2 FIXED)\n",
            "==========================================================================================\n",
            "\n",
            "                                   Model  Accuracy  Sensitivity  Specificity  Precision  F1-Score   G-Mean  AUC-ROC\n",
            "LogisticRegression (Fixed Sampling + BO)  0.643678        0.550     0.664789   0.269939  0.362140 0.604677 0.617887\n",
            "          AdaBoost (Fixed Sampling + BO)  0.816092        0.650     0.853521   0.500000  0.565217 0.744841 0.808908\n",
            "           XGBoost (Fixed Sampling + BO)  0.735632        0.475     0.794366   0.342342  0.397906 0.614267 0.698556\n",
            "\n",
            "==========================================================================================\n",
            " üìä PERBANDINGAN LENGKAP: Semua Eksperimen\n",
            "==========================================================================================\n",
            "\n",
            "                                   Model  Accuracy  Sensitivity  Specificity  Precision  F1-Score   G-Mean  AUC-ROC\n",
            "          Logistic Regression (Baseline)  0.816092       0.0000     1.000000   0.000000  0.000000 0.000000 0.614366\n",
            "                     AdaBoost (Baseline)  0.827586       0.3125     0.943662   0.555556  0.400000 0.543042 0.791532\n",
            "                      XGBoost (Baseline)  0.786207       0.3375     0.887324   0.402985  0.367347 0.547240 0.713556\n",
            "  Logistic Regression (SMOTE-NC Default)  0.701149       0.4500     0.757746   0.295082  0.356436 0.583940 0.623592\n",
            "             AdaBoost (SMOTE-NC Default)  0.813793       0.6375     0.853521   0.495146  0.557377 0.737645 0.813592\n",
            "              XGBoost (SMOTE-NC Default)  0.754023       0.4750     0.816901   0.368932  0.415301 0.622919 0.723838\n",
            "LogisticRegression (Fixed Sampling + BO)  0.643678       0.5500     0.664789   0.269939  0.362140 0.604677 0.617887\n",
            "          AdaBoost (Fixed Sampling + BO)  0.816092       0.6500     0.853521   0.500000  0.565217 0.744841 0.808908\n",
            "           XGBoost (Fixed Sampling + BO)  0.735632       0.4750     0.794366   0.342342  0.397906 0.614267 0.698556\n",
            "\n",
            "==========================================================================================\n",
            " üèÜ MODEL TERBAIK KESELURUHAN: AdaBoost (Fixed Sampling + BO)\n",
            "==========================================================================================\n",
            "   G-Mean Score: 0.7448\n",
            "   Sensitivity : 0.6500\n",
            "   Specificity : 0.8535\n",
            "   Accuracy    : 0.8161\n",
            "   AUC-ROC     : 0.8089\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            " üìà IMPROVEMENT ANALYSIS: Default vs BO (FIXED)\n",
            "==========================================================================================\n",
            "\n",
            "Model                      Default G-Mean       BO G-Mean     Improvement               Status\n",
            "-----------------------------------------------------------------------------------------------\n",
            "Logistic Regression                0.5839          0.6047          +3.55%       ‚úÖ IMPROVEMENT!\n",
            "AdaBoost                           0.7376          0.7448          +0.98% ‚úì Slight Improvement\n",
            "XGBoost                            0.6229          0.6143          -1.39%       ‚ö†Ô∏è DEGRADATION\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            " üîç VERIFICATION: Cek Hasil Identik\n",
            "==========================================================================================\n",
            "\n",
            "üìã Detailed Comparison untuk setiap model:\n",
            "\n",
            "\n",
            "Logistic Regression:\n",
            "--------------------------------------------------------------------------------\n",
            "  Accuracy       : Default=0.7011, BO=0.6437, Diff=0.0575 ‚úÖ Berbeda Signifikan\n",
            "  Sensitivity    : Default=0.4500, BO=0.5500, Diff=0.1000 ‚úÖ Berbeda Signifikan\n",
            "  Specificity    : Default=0.7577, BO=0.6648, Diff=0.0930 ‚úÖ Berbeda Signifikan\n",
            "  Precision      : Default=0.2951, BO=0.2699, Diff=0.0251 ‚úÖ Berbeda Signifikan\n",
            "  F1-Score       : Default=0.3564, BO=0.3621, Diff=0.0057 ‚úì Perbedaan Kecil\n",
            "  G-Mean         : Default=0.5839, BO=0.6047, Diff=0.0207 ‚úÖ Berbeda Signifikan\n",
            "  AUC-ROC        : Default=0.6236, BO=0.6179, Diff=0.0057 ‚úì Perbedaan Kecil\n",
            "\n",
            "  TOTAL METRIK IDENTIK: 0/7\n",
            "  ‚úÖ BAGUS: Tidak ada metrik yang identik - BO bekerja dengan baik!\n",
            "\n",
            "AdaBoost:\n",
            "--------------------------------------------------------------------------------\n",
            "  Accuracy       : Default=0.8138, BO=0.8161, Diff=0.0023 ‚úì Perbedaan Kecil\n",
            "  Sensitivity    : Default=0.6375, BO=0.6500, Diff=0.0125 ‚úÖ Berbeda Signifikan\n",
            "  Specificity    : Default=0.8535, BO=0.8535, Diff=0.0000 ‚ö†Ô∏è IDENTIK\n",
            "  Precision      : Default=0.4951, BO=0.5000, Diff=0.0049 ‚úì Perbedaan Kecil\n",
            "  F1-Score       : Default=0.5574, BO=0.5652, Diff=0.0078 ‚úì Perbedaan Kecil\n",
            "  G-Mean         : Default=0.7376, BO=0.7448, Diff=0.0072 ‚úì Perbedaan Kecil\n",
            "  AUC-ROC        : Default=0.8136, BO=0.8089, Diff=0.0047 ‚úì Perbedaan Kecil\n",
            "\n",
            "  TOTAL METRIK IDENTIK: 1/7\n",
            "  ‚úì OK: Beberapa perbedaan terdeteksi\n",
            "\n",
            "XGBoost:\n",
            "--------------------------------------------------------------------------------\n",
            "  Accuracy       : Default=0.7540, BO=0.7356, Diff=0.0184 ‚úÖ Berbeda Signifikan\n",
            "  Sensitivity    : Default=0.4750, BO=0.4750, Diff=0.0000 ‚ö†Ô∏è IDENTIK\n",
            "  Specificity    : Default=0.8169, BO=0.7944, Diff=0.0225 ‚úÖ Berbeda Signifikan\n",
            "  Precision      : Default=0.3689, BO=0.3423, Diff=0.0266 ‚úÖ Berbeda Signifikan\n",
            "  F1-Score       : Default=0.4153, BO=0.3979, Diff=0.0174 ‚úÖ Berbeda Signifikan\n",
            "  G-Mean         : Default=0.6229, BO=0.6143, Diff=0.0087 ‚úì Perbedaan Kecil\n",
            "  AUC-ROC        : Default=0.7238, BO=0.6986, Diff=0.0253 ‚úÖ Berbeda Signifikan\n",
            "\n",
            "  TOTAL METRIK IDENTIK: 1/7\n",
            "  ‚úì OK: Beberapa perbedaan terdeteksi\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            " üíæ SAVE RESULTS\n",
            "==========================================================================================\n",
            "\n",
            "‚úÖ Results saved: results/all_results_comprehensive.csv\n",
            "‚úÖ Results saved: results/baseline_results.csv\n",
            "‚úÖ Results saved: results/smote_nc_default_results.csv\n",
            "‚úÖ Results saved: results/fixed_sampling_optimized_results.csv\n",
            "\n",
            "==========================================================================================\n",
            " ‚úÖ‚úÖ‚úÖ ALL EXPERIMENTS COMPLETED! ‚úÖ‚úÖ‚úÖ\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BONUS: SUMMARY TABLE - BEST PARAMETERS FROM BO\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" üìã SUMMARY: BEST PARAMETERS FROM BAYESIAN OPTIMIZATION\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Create summary\n",
        "summary_data = []\n",
        "\n",
        "# Logistic Regression\n",
        "summary_data.append({\n",
        "    'Model': 'Logistic Regression',\n",
        "    'k_neighbors': int(round(lr_best_params['k_neighbors'])),\n",
        "    'sampling_strategy': f\"{lr_best_params['sampling_strategy']:.4f}\",\n",
        "    'Model_Param_1': f\"C={lr_best_params['C']:.4f}\",\n",
        "    'Model_Param_2': '-',\n",
        "    'Model_Param_3': '-',\n",
        "    'CV_G-Mean': f\"{lr_best_score:.4f}\",\n",
        "    'Test_G-Mean': f\"{result_lr_bo['G-Mean']:.4f}\"\n",
        "})\n",
        "\n",
        "# AdaBoost\n",
        "summary_data.append({\n",
        "    'Model': 'AdaBoost',\n",
        "    'k_neighbors': int(round(ada_best_params['k_neighbors'])),\n",
        "    'sampling_strategy': f\"{ada_best_params['sampling_strategy']:.4f}\",\n",
        "    'Model_Param_1': f\"n_est={int(round(ada_best_params['n_estimators']))}\",\n",
        "    'Model_Param_2': f\"lr={ada_best_params['learning_rate']:.4f}\",\n",
        "    'Model_Param_3': '-',\n",
        "    'CV_G-Mean': f\"{ada_best_score:.4f}\",\n",
        "    'Test_G-Mean': f\"{result_ada_bo['G-Mean']:.4f}\"\n",
        "})\n",
        "\n",
        "# XGBoost\n",
        "summary_data.append({\n",
        "    'Model': 'XGBoost',\n",
        "    'k_neighbors': int(round(xgb_best_params['k_neighbors'])),\n",
        "    'sampling_strategy': f\"{xgb_best_params['sampling_strategy']:.4f}\",\n",
        "    'Model_Param_1': f\"n_est={int(round(xgb_best_params['n_estimators']))}\",\n",
        "    'Model_Param_2': f\"depth={int(round(xgb_best_params['max_depth']))}\",\n",
        "    'Model_Param_3': f\"lr={xgb_best_params['learning_rate']:.4f}\",\n",
        "    'CV_G-Mean': f\"{xgb_best_score:.4f}\",\n",
        "    'Test_G-Mean': f\"{result_xgb_bo['G-Mean']:.4f}\"\n",
        "})\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"\\n\" + df_summary.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"\\nüí° KEY OBSERVATIONS:\")\n",
        "print(\"-\" * 90)\n",
        "print(f\"  1. sampling_strategy bervariasi untuk setiap model:\")\n",
        "for i, row in df_summary.iterrows():\n",
        "    print(f\"     - {row['Model']}: {row['sampling_strategy']}\")\n",
        "\n",
        "print(f\"\\n  2. Perbedaan dari default (0.8):\")\n",
        "for i, row in df_summary.iterrows():\n",
        "    ss_val = float(row['sampling_strategy'])\n",
        "    diff = abs(ss_val - 0.8)\n",
        "    print(f\"     - {row['Model']}: {diff:.4f} {'‚úÖ BERBEDA' if diff > 0.01 else '‚ö†Ô∏è MIRIP'}\")\n",
        "\n",
        "print(f\"\\n  3. CV G-Mean vs Test G-Mean:\")\n",
        "for i, row in df_summary.iterrows():\n",
        "    cv_val = float(row['CV_G-Mean'])\n",
        "    test_val = float(row['Test_G-Mean'])\n",
        "    diff = test_val - cv_val\n",
        "    print(f\"     - {row['Model']}: {diff:+.4f} {'‚úÖ Consistent' if abs(diff) < 0.05 else '‚ö†Ô∏è Gap'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "\n",
        "# Save summary\n",
        "df_summary.to_csv('results/best_parameters_summary.csv', index=False)\n",
        "print(\"\\n‚úÖ Summary saved: results/best_parameters_summary.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\" üéâüéâüéâ SEMUA EKSPERIMEN SELESAI! üéâüéâüéâ\")\n",
        "print(\"=\"*90)\n",
        "print(\"\\nüìÅ Semua hasil tersimpan di folder 'results/':\")\n",
        "print(\"   ‚úì all_results_comprehensive.csv\")\n",
        "print(\"   ‚úì baseline_results.csv\")\n",
        "print(\"   ‚úì smote_nc_default_results.csv\")\n",
        "print(\"   ‚úì fixed_sampling_optimized_results.csv\")\n",
        "print(\"   ‚úì best_parameters_summary.csv\")\n",
        "print(\"   ‚úì gmean_comparison_all_experiments.png\")\n",
        "print(\"\\nüöÄ Anda dapat menggunakan hasil ini untuk analisis tesis Anda!\")\n",
        "print(\"=\"*90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_UQFsVvqvk7",
        "outputId": "992788f5-b342-4411-f2b4-a9fb5c50d879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            " üìã SUMMARY: BEST PARAMETERS FROM BAYESIAN OPTIMIZATION\n",
            "==========================================================================================\n",
            "\n",
            "              Model  k_neighbors sampling_strategy Model_Param_1 Model_Param_2 Model_Param_3 CV_G-Mean Test_G-Mean\n",
            "Logistic Regression            3            0.9331     C=60.1119             -             -    0.6165      0.6047\n",
            "           AdaBoost            6            0.8546     n_est=150     lr=1.1641             -    0.7328      0.7448\n",
            "            XGBoost            6            0.9754     n_est=233       depth=7     lr=0.0552    0.8005      0.6143\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "üí° KEY OBSERVATIONS:\n",
            "------------------------------------------------------------------------------------------\n",
            "  1. sampling_strategy bervariasi untuk setiap model:\n",
            "     - Logistic Regression: 0.9331\n",
            "     - AdaBoost: 0.8546\n",
            "     - XGBoost: 0.9754\n",
            "\n",
            "  2. Perbedaan dari default (0.8):\n",
            "     - Logistic Regression: 0.1331 ‚úÖ BERBEDA\n",
            "     - AdaBoost: 0.0546 ‚úÖ BERBEDA\n",
            "     - XGBoost: 0.1754 ‚úÖ BERBEDA\n",
            "\n",
            "  3. CV G-Mean vs Test G-Mean:\n",
            "     - Logistic Regression: -0.0118 ‚úÖ Consistent\n",
            "     - AdaBoost: +0.0120 ‚úÖ Consistent\n",
            "     - XGBoost: -0.1862 ‚ö†Ô∏è Gap\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "‚úÖ Summary saved: results/best_parameters_summary.csv\n",
            "\n",
            "==========================================================================================\n",
            " üéâüéâüéâ SEMUA EKSPERIMEN SELESAI! üéâüéâüéâ\n",
            "==========================================================================================\n",
            "\n",
            "üìÅ Semua hasil tersimpan di folder 'results/':\n",
            "   ‚úì all_results_comprehensive.csv\n",
            "   ‚úì baseline_results.csv\n",
            "   ‚úì smote_nc_default_results.csv\n",
            "   ‚úì fixed_sampling_optimized_results.csv\n",
            "   ‚úì best_parameters_summary.csv\n",
            "   ‚úì gmean_comparison_all_experiments.png\n",
            "\n",
            "üöÄ Anda dapat menggunakan hasil ini untuk analisis tesis Anda!\n",
            "==========================================================================================\n"
          ]
        }
      ]
    }
  ]
}